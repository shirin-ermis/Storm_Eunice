{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "import random \n",
    "import scipy as sc\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats\n",
    "import cartopy.crs as ccrs\n",
    "import pygrib\n",
    "import cfgrib\n",
    "import stormeunice as eun\n",
    "\n",
    "sns.set_theme(style=\"white\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "random.seed(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data in pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast data for friday in south UK\n",
    "directory, experiments, inits, cfpf = eun.data.Data.load_meta()\n",
    "lat, lon = eun.data.Data.get_latlon()\n",
    "south_df = eun.data.Data.get_friday_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cumulative wind gusts and wind gust histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum in time for each grid box\n",
    "max_time = south_df.groupby(['lat', 'lon', 'init', 'member']).max() # inits are different for pi and incr as well\n",
    "max_time_mean_latlon = max_time.groupby(['experiment','init', 'member']).mean(numeric_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "sns.ecdfplot(max_time_mean_latlon, x = 'fg10', hue = 'experiment')\n",
    "plt.xlabel('Wind gusts at 10 m/s')\n",
    "plt.ylabel('Probability of exceedance')\n",
    "sns.despine()\n",
    "plt.title('Wind gusts in southern UK: maximum in time, average over each member')\n",
    "\n",
    "plt.savefig('figures/03_averaged_cumulative_wind_gusts.pdf')\n",
    "plt.savefig('figures/03_averaged_cumulative_wind_gusts.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "south_df['experiment'] = pd.Categorical(south_df['experiment'], ['curr','incr','pi'])\n",
    "sns.histplot(south_df, x = 'fg10', hue = 'experiment', bins = 20)\n",
    "plt.xlabel('Wind gusts at 10 m/s')\n",
    "plt.ylabel('Probability of exceedance')\n",
    "sns.despine()\n",
    "plt.title('Wind gusts in southern UK on Friday, 18th Feb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wind gusts by initialisation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset index from groupby object to variable \n",
    "# https://stackoverflow.com/questions/10373660/converting-a-pandas-groupby-output-from-series-to-dataframe\n",
    "\n",
    "max_time_mean_latlon_ = max_time_mean_latlon.reset_index()\n",
    "max_time_mean_latlon_ = max_time_mean_latlon_.assign(init_date = max_time_mean_latlon_.init.str[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_fits = {}\n",
    "\n",
    "for experiment in experiments:\n",
    "    for init in inits[experiment]:\n",
    "        \n",
    "        weibull_fits[init] = {'x' : [], 'params' : [], 'rv' : []}\n",
    "        data = max_time_mean_latlon_[max_time_mean_latlon_.init == init]\n",
    "        shape, loc, shape1 = stats.weibull_min.fit(data.fg10.values, floc = 10)\n",
    "        rv_fit = stats.weibull_min(shape, loc, shape1)  # frozen pdf\n",
    "        weibull_fits[init]['rv'] = rv_fit\n",
    "        weibull_fits[init]['x'] = np.linspace(rv_fit.ppf(0.01), rv_fit.ppf(0.99), 1000)\n",
    "        weibull_fits[init]['params'] = [shape, loc, shape1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette(\"colorblind\") # could also use palette = sns.color_palette(\"Paired\", 9)\n",
    "g = sns.FacetGrid(max_time_mean_latlon_, col = 'init_date', hue = 'experiment')\n",
    "for i, ax in enumerate(g.axes_dict.values()):\n",
    "    for experiment in experiments:\n",
    "        init = inits[experiment][i]\n",
    "        x = weibull_fits[init]['x']\n",
    "        rv = weibull_fits[init]['rv']\n",
    "        #ax.plot(x, rv.cdf(x), color = 'grey', lw = 1)\n",
    "        sns.lineplot(x = x, y= rv.cdf(x), ax = ax, color = 'grey', lw = 1)\n",
    "g.map(sns.ecdfplot,'fg10')\n",
    "g.set_axis_labels(\"Wind gusts at 10 m/s\", \"Probability of exceedance\")\n",
    "plt.xlim(8, 45)\n",
    "g.add_legend()\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bootstrap 1,000 times with samples of size 1,000\n",
    "\n",
    "init_dates = ['2022-02-10', '2022-02-14', '2022-02-16']\n",
    "bootstraps = 1000\n",
    "sample_size = 51\n",
    "\n",
    "for init_date in init_dates:\n",
    "    print('Initialisation date: ' ,init_date)\n",
    "    pis = max_time_mean_latlon_[(max_time_mean_latlon_.init_date == init_date) & (max_time_mean_latlon_.experiment == 'pi')].fg10.values\n",
    "    currs = max_time_mean_latlon_[(max_time_mean_latlon_.init_date == init_date) & (max_time_mean_latlon_.experiment == 'curr')].fg10.values\n",
    "    incrs = max_time_mean_latlon_[(max_time_mean_latlon_.init_date == init_date) & (max_time_mean_latlon_.experiment == 'incr')].fg10.values\n",
    "\n",
    "    # lists for p values\n",
    "    pi_curr = []\n",
    "    pi_incr = []\n",
    "    curr_incr = []\n",
    "\n",
    "    for i in range(bootstraps):\n",
    "        pi = np.random.choice(pis, size = sample_size)\n",
    "        curr = np.random.choice(currs, size = sample_size)\n",
    "        incr = np.random.choice(incrs, size = sample_size)\n",
    "\n",
    "        pi_curr.append(stats.kstest(pi, curr)[1])\n",
    "        pi_incr.append(stats.kstest(pi, incr)[1])\n",
    "        curr_incr.append(stats.kstest(curr, incr)[1])\n",
    "    print('PI v CURR: p_se = ', np.std(pi_curr)/np.sqrt(bootstraps), ', full sample: p = ', stats.kstest(pis,currs)[1])\n",
    "    print('PI v INCR: p_se = ', np.std(pi_incr)/np.sqrt(bootstraps), ', full sample: p = ', stats.kstest(pis,incrs)[1])\n",
    "    print('CURR v INCR: p_se = ', np.std(curr_incr)/np.sqrt(bootstraps), ', full sample: p = ',stats.kstest(currs, incrs)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(max_time_mean_latlon_, col = 'init_date', hue = 'experiment')\n",
    "g.map(sns.histplot,'fg10', bins = 20, stat='density')\n",
    "g.set_axis_labels(\"Wind gusts at 10 m/s\", \"Proportion\")\n",
    "for i, ax in enumerate(g.axes_dict.values()):\n",
    "    for experiment in experiments:\n",
    "        init = inits[experiment][i]\n",
    "        x = weibull_fits[init]['x']\n",
    "        rv = weibull_fits[init]['rv']\n",
    "        ax.plot(x, rv.pdf(x), color = 'grey')\n",
    "g.add_legend()\n",
    "plt.xlim(8, 45)\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different distributions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Weibull (floc = 0,10), generalised logistic, generalised extreme value "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cram√©r von Mises test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weibull_fits0 = {}\n",
    "weibull_fits10 = {}\n",
    "gev_fits = {}\n",
    "gl_fits = {}\n",
    "bootstraps = 1000\n",
    "sample_size = 51\n",
    "\n",
    "args = {'weibull0': [], 'weibull10': [], 'gev': [], 'gl': []}\n",
    "\n",
    "\n",
    "for experiment in experiments:\n",
    "    for init in inits[experiment]:\n",
    "        \n",
    "        weibull_fits0[init] = {'p_boot' : [], 'p_full' : [], 'KS_boot' : [], 'KS_full' : []}\n",
    "        weibull_fits10[init] = {'p_boot' : [], 'p_full' : [], 'KS_boot' : [], 'KS_full' : []}\n",
    "        gev_fits[init] = {'p_boot' : [], 'p_full' : [], 'KS_boot' : [], 'KS_full' : []}\n",
    "        gl_fits[init] = {'p_boot' : [], 'p_full' : [], 'KS_boot' : [], 'KS_full' : []}\n",
    "\n",
    "        data = max_time_mean_latlon_[max_time_mean_latlon_.init == init]\n",
    "        args['weibull0'] = stats.weibull_min.fit(data.fg10.values, floc = 0)\n",
    "        args['weibull10'] = stats.weibull_min.fit(data.fg10.values, floc = 10)\n",
    "        args['gev'] = stats.genextreme.fit(data.fg10.values)\n",
    "        args['gl'] = stats.genlogistic.fit(data.fg10.values)\n",
    "        \n",
    "        for i in range(bootstraps):\n",
    "            sample = np.random.choice(data.fg10.values, size = sample_size)\n",
    "            weibull0_ks = stats.cramervonmises(sample, \"weibull_min\", args = args['weibull0'])\n",
    "            weibull10_ks = stats.cramervonmises(sample, \"weibull_min\", args = args['weibull10'])\n",
    "            gev_ks = stats.cramervonmises(sample, \"genextreme\", args = args['gev'])\n",
    "            gl_ks = stats.cramervonmises(sample, \"genlogistic\", args = args['gl'])\n",
    "\n",
    "            weibull_fits0[init]['p_boot'].append(weibull0_ks.pvalue)\n",
    "            weibull_fits10[init]['p_boot'].append(weibull10_ks.pvalue)\n",
    "            gev_fits[init]['p_boot'].append(gev_ks.pvalue)\n",
    "            gl_fits[init]['p_boot'].append(gl_ks.pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for experiment in experiments:\n",
    "    print(experiment)\n",
    "    for init in inits[experiment]:\n",
    "    #     print(init,' Weibull 0 p = ', np.mean(np.array(weibull_fits0[init]['p_boot'])))\n",
    "        print(init,' Weibull 10 p = ', np.mean(np.array(weibull_fits10[init]['p_boot'])))\n",
    "        # print(init,' GEV p = ', np.mean(np.array(gev_fits[init]['p_boot'])))\n",
    "        # print(init,' GL 0 p = ', np.mean(np.array(gl_fits[init]['p_boot'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(obs, rv): \n",
    "    \"\"\"\n",
    "    Function to calculate negative likelihood of a fit for a pdf to data\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    obs: array, sample data\n",
    "    rv: fitted function, scipy frozen distribution object\n",
    "\n",
    "    Outputs\n",
    "    -------\n",
    "    negative log likelihood\n",
    "    \"\"\"\n",
    "\n",
    "    import scipy.stats as stat\n",
    "\n",
    "    # old version\n",
    "    # curr = 1\n",
    "    # curr = [(curr:=curr*rv.pdf(v)) for v in obs]\n",
    "    # return -np.log(curr[-1])\n",
    "\n",
    "    return -1*np.log(rv.pdf(obs)).sum()  # Nick's idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = 0.1\n",
    "args = (1,loc,1)\n",
    "data = np.array([0.2,2,2, 0.1]) # assume for now this is my wind gust data\n",
    "rv = stats.weibull_min(*args)\n",
    "print('neg log likelihood = ', NLL(data, rv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summed_NLL(loc, *args):  # args in order (obs, rv, scale_shape)\n",
    "    curr = 0  # for the sum \n",
    "\n",
    "    for experiment in experiments:\n",
    "        for init in inits[experiment]:\n",
    "            rv = args[1](args[2][experiment][init][0],loc, args[2][experiment][init][1])\n",
    "            data = args[0][max_time_mean_latlon_.init == init].fg10.values\n",
    "            curr += NLL(data, rv)\n",
    "   \n",
    "    # Calculate neg log likelihood\n",
    "    return -np.log(curr)\n",
    "\n",
    "\n",
    "def summed_NLL2(loc, *args):  # args in order (obs, rv, scale_shape)\n",
    "    curr = 0  # for the sum \n",
    "\n",
    "    for experiment in experiments:\n",
    "        rv = args[1](args[2][experiment][0],loc, args[2][experiment][1])\n",
    "        data = args[0][max_time_mean_latlon_.experiment == experiment].fg10.values\n",
    "        curr += NLL(data, rv)\n",
    "   \n",
    "    # Calculate neg log likelihood\n",
    "    return -np.log(curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize scale and shape for all ensembles using fixed location parameter\n",
    "\n",
    "dists = [stats.weibull_min, stats.genextreme, stats.genlogistic]\n",
    "dist_names = ['Weibull', 'GEV', 'GenLogistic']\n",
    "\n",
    "for i, dist in enumerate(dists):\n",
    "    loc = 10\n",
    "    scale_shape = {}\n",
    "    for experiment in experiments:\n",
    "        scale_shape[experiment] = {}\n",
    "        for init in inits[experiment]:\n",
    "            data = max_time_mean_latlon_[max_time_mean_latlon_.init == init].fg10.values\n",
    "            x0 = [weibull_fits[init]['params'][0], weibull_fits[init]['params'][2]]\n",
    "            scale_shape[experiment][init] = sc.optimize.minimize(lambda x : NLL(data, stats.weibull_min(x[0],loc, x[1])), \n",
    "                                            x0 = x0, \n",
    "                                            method = \"Nelder-Mead\", \n",
    "                                            options = {'maxfev' : 100}).x\n",
    "\n",
    "    # Now minimise the location parameter  for all ensembles using the shape and scale from above   \n",
    "\n",
    "    loc = sc.optimize.minimize(summed_NLL, \n",
    "                            args = (max_time_mean_latlon_, stats.weibull_min, scale_shape),\n",
    "                            x0 = 10, \n",
    "                            method = \"Nelder-Mead\", \n",
    "                            options = {'maxfev' : 100}).x\n",
    "    print(dist_names[i],': NLL = ', summed_NLL(loc, max_time_mean_latlon_, stats.weibull_min, scale_shape), ', loc =' ,loc[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only fit scale and shape for experiments not each ensemble separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimize scale and shape for all experiments using fixed location parameter\n",
    "\n",
    "dists = [stats.weibull_min, stats.genextreme, stats.genlogistic]\n",
    "dist_names = ['Weibull', 'GEV', 'GenLogistic']\n",
    "\n",
    "for i, dist in enumerate(dists):\n",
    "    loc = 10\n",
    "    scale_shape = {}\n",
    "    for experiment in experiments:\n",
    "        scale_shape[experiment] = {}\n",
    "        data = max_time_mean_latlon_[max_time_mean_latlon_.init == init].fg10.values\n",
    "        x0 = [weibull_fits[init]['params'][0], weibull_fits[init]['params'][2]]\n",
    "        scale_shape[experiment] = sc.optimize.minimize(lambda x : NLL(data, stats.weibull_min(x[0],loc, x[1])), \n",
    "                                                       x0 = x0, \n",
    "                                                       method = \"Nelder-Mead\", \n",
    "                                                       options = {'maxfev' : 100}).x\n",
    "\n",
    "    # Now minimise the location parameter  for all ensembles using the shape and scale from above   \n",
    "\n",
    "    loc = sc.optimize.minimize(summed_NLL2, \n",
    "                            args = (max_time_mean_latlon_, stats.weibull_min, scale_shape),\n",
    "                            x0 = 10, \n",
    "                            method = \"Nelder-Mead\", \n",
    "                            options = {'maxfev' : 100}).x\n",
    "    print(dist_names[i],': NLL = ', summed_NLL2(loc, max_time_mean_latlon_, stats.weibull_min, scale_shape), ', loc =' ,loc[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "storm_eunice39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "e85e7c9027c9ef342f521c50884794b3ff4e0d77b330915340a6a92aa790fc1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
